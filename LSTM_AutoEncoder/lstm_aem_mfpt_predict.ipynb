{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12fdac0",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ebd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1895b",
   "metadata": {},
   "source": [
    "# Dataset Path Setting\n",
    "make sure the paths are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets:\n",
    "    MFPT_BASE = \"/media/sda1/dataset/MFPT/MFPT Fault Data Sets\"\n",
    "    MFPT_DATA = \"1 - Three Baseline Conditions\"\n",
    "    MFPT_MAT = \"baseline_1.mat\"\n",
    "    MFPT_ABNORMAL_DATA = \"2 - Three Outer Race Fault Conditions\"\n",
    "    MFPT_ABNORMAL_MAT = \"OuterRaceFault_1.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927e091",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Preprocessing datas before the training\n",
    "Simply change dataset shape in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataset(data, window_size, preprocess=False):\n",
    "        org_shape = data.shape\n",
    "        truncated = data.size - (int(data.size/window_size) * window_size)\n",
    "        data = data[:data.size - truncated]\n",
    "        \n",
    "        if preprocess:\n",
    "            minmax = MinMaxScaler()\n",
    "            data = minmax.fit_transform(data)\n",
    "        dataset = np.empty((int(data.size/window_size), window_size, 1))\n",
    "        j = 0\n",
    "        for i in [n*window_size for n in range(int(data.size/window_size))]:\n",
    "            dataset[j] = data[i:i+window_size]\n",
    "            j = j + 1\n",
    "            \n",
    "        print(f\"Dataset shape changed {org_shape} -> {dataset.shape}.\")\n",
    "        print(f\"Truncated {truncated} values \")\n",
    "        #print(f\"{data[data.size-100:data.size-50]}, {dataset[-2]}\")\n",
    "        #print(f\"{data[data.size-50:data.size]}, {dataset[-1]}\")\n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_overlap_dataset(data, window_size, preprocess=False):\n",
    "        org_shape = data.shape\n",
    "        if preprocess:\n",
    "            minmax = MinMaxScaler()\n",
    "            data = minmax.fit_transform(data)\n",
    "        dataset = np.empty((data.size-window_size+1, window_size, 1))\n",
    "        j = 0\n",
    "        for i in range(data.size-window_size+1):\n",
    "            dataset[j] = data[i:i+window_size]\n",
    "            j = j + 1\n",
    "        print(f\"Dataset shape changed {org_shape} -> {dataset.shape}\")\n",
    "        #print(f\"{data[data.size-100:data.size-50]}, {dataset[-2]}\")\n",
    "        #print(f\"{data[data.size-50:data.size]}, {dataset[-1]}\")\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa05d5",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23e6df",
   "metadata": {},
   "source": [
    "### Load Anomaly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec49691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(Datasets.MFPT_BASE, Datasets.MFPT_ABNORMAL_DATA, Datasets.MFPT_ABNORMAL_MAT)\n",
    "\n",
    "mat = loadmat(path)\n",
    "keys = mat['bearing'].dtype.fields\n",
    "mat_dict = {\n",
    "    list(keys.keys())[0] : mat['bearing'][0][0][0],\n",
    "    list(keys.keys())[1] : mat['bearing'][0][0][1],\n",
    "    list(keys.keys())[2] : mat['bearing'][0][0][2],\n",
    "    list(keys.keys())[3] : mat['bearing'][0][0][3]\n",
    "}\n",
    "anomaly_dataset = Preprocessing.create_dataset(mat_dict['gs'], 50, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b6d12",
   "metadata": {},
   "source": [
    "### Load Normal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619da9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(Datasets.MFPT_BASE, Datasets.MFPT_DATA, Datasets.MFPT_MAT)\n",
    "\n",
    "mat = loadmat(path)\n",
    "keys = mat['bearing'].dtype.fields\n",
    "mat_dict = {\n",
    "    list(keys.keys())[0] : mat['bearing'][0][0][0],\n",
    "    list(keys.keys())[1] : mat['bearing'][0][0][1],\n",
    "    list(keys.keys())[2] : mat['bearing'][0][0][2],\n",
    "    list(keys.keys())[3] : mat['bearing'][0][0][3]\n",
    "}\n",
    "normal_dataset = Preprocessing.create_dataset(mat_dict['gs'], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21785579",
   "metadata": {},
   "source": [
    "# Load Model & Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736c665",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./results\"\n",
    "MODEL_NAME = \"MFPT_lstmAE_20_50_2022_02_03_20:11\"\n",
    "model = keras.models.load_model(f'{DIR}/{MODEL_NAME}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90d292",
   "metadata": {},
   "source": [
    "### Calculate Loss for Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_loss_list = []\n",
    "i = 0\n",
    "print(\"evaluating...\")\n",
    "for data in anomaly_dataset:\n",
    "    data = np.reshape(data, (1,anomaly_dataset.shape[1], anomaly_dataset.shape[2]))\n",
    "    loss = model.evaluate(data, data, verbose=0)\n",
    "    anomaly_loss_list.append(loss)\n",
    "    print(f\"{i} / {anomaly_dataset.shape[0]} | {int(i / anomaly_dataset.shape[0] * 100)}%\", end = '\\r')\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0a912",
   "metadata": {},
   "source": [
    "### Calculate Loss For Normal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63439bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_loss_list = []\n",
    "i = 0\n",
    "print(\"evaluating...\")\n",
    "for data in normal_dataset:\n",
    "    data = np.reshape(data, (1,normal_dataset.shape[1], normal_dataset.shape[2]))\n",
    "    loss = model.evaluate(data, data, verbose=0)\n",
    "    normal_loss_list.append(loss)\n",
    "    print(f\"{i} / {normal_dataset.shape[0]} | {int(i / normal_dataset.shape[0] * 100)}%\", end = '\\r')\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c67861",
   "metadata": {},
   "source": [
    "### Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11351eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_loss_list = np.array(normal_loss_list)\n",
    "np.save(f'{DIR}/{MODEL_NAME}/{MODEL_NAME}_{Datasets.MFPT_MAT}_Normal_losses.npy', normal_loss_list)\n",
    "\n",
    "anomaly_loss_list = np.array(anomaly_loss_list)\n",
    "np.save(f'{DIR}/{MODEL_NAME}/{MODEL_NAME}_{Datasets.MFPT_ABNORMAL_MAT}_losses.npy', anomaly_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4ab2f",
   "metadata": {},
   "source": [
    "# Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./results\"\n",
    "MODEL_NAME = \"MFPT_lstmAE_10_50_2022_01_25_15:54\"\n",
    "NORMAL_FILE_NAME = \"MFPT_lstmAE_10_50_2022_01_25_15:54_baseline_1.mat_Normal_losses.npy\"\n",
    "ANOMALY_FILE_NAME = \"MFPT_lstmAE_10_50_2022_01_25_15:54_OuterRaceFault_1.mat_losses.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./results\"\n",
    "MODEL_NAME = \"MFPT_lstmAE_5_50_2022_01_25_15:34\"\n",
    "NORMAL_FILE_NAME = \"MFPT_lstmAE_5_50_2022_01_25_15:34_baseline_1.mat_Normal_losses.npy\"\n",
    "ANOMALY_FILE_NAME = \"MFPT_lstmAE_5_50_2022_01_25_15:34_OuterRaceFault_1.mat_losses.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_loss_list = np.load(f'{DIR}/{MODEL_NAME}/{NORMAL_FILE_NAME}')\n",
    "anomaly_loss_list = np.load(f'{DIR}/{MODEL_NAME}/{ANOMALY_FILE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868054a5",
   "metadata": {},
   "source": [
    "# Set Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bae658",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(normal_loss_list) + 2*np.std(normal_loss_list)\n",
    "threshold_max = np.array(normal_loss_list).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555f9b1",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81895c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (20,10)) # This is your answer to resize the figure\n",
    "# The below will help you expand on your question and resize individual elements within your figure. Experiement with the below parameters.\n",
    "axs.set_title(MODEL_NAME, fontsize = 17.5)\n",
    "axs.tick_params(axis = 'x', labelsize = 14)\n",
    "axs.set_xlabel('Timestamp', size = 15)\n",
    "axs.tick_params(axis = 'y', labelsize =14)\n",
    "axs.set_ylabel('Loss', size = 15)\n",
    "plt.plot(anomaly_loss_list, 'r', linewidth=1, label=\"Anomaly\")\n",
    "plt.plot(normal_loss_list, 'g', linewidth=1, label=\"Normal\")\n",
    "plt.hlines(threshold,0,np.array(normal_loss_list).size,'y', label=\"Threshold\")\n",
    "plt.hlines(threshold_max,0,np.array(normal_loss_list).size,'y', label=\"Normal Max\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{DIR}/{MODEL_NAME}/{MODEL_NAME}_Anomaly_Loss_Graph.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29c70647f7aba533f8324fd25b74ed801e1c1932a37b2928b45f73b05717af80"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
