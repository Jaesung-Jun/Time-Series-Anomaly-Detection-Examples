{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('./data/IP/DHCP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = data_frame['Svr_detect'].values + data_frame['Svr_connect'].values + data_frame['Ss_request'].values\n",
    "data_frame['server'] = server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.drop(['Svr_detect', 'Svr_connect', 'Ss_request'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_label = pd.read_csv('./server_label.csv')['y'].fillna(0)\n",
    "ss_label = pd.read_csv('./ss_label.csv')['y'].fillna(0)\n",
    "server_label = server_label.values.astype(bool)\n",
    "ss_label = ss_label.values.astype(bool)\n",
    "data_frame['y'] = np.logical_or(server_label, ss_label).astype(float)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['server'].plot(figsize=(30,12))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(np.arange(0, 52560, 1000),fontsize=15,rotation=90)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_half = data_frame.index[data_frame['Timestamp'] == '20210630_2350-0000'].tolist()[0]\n",
    "#print(idx_half)\n",
    "#train_data = data_frame[:idx_half+1]\n",
    "#test_data = data_frame[idx_half+1:]\n",
    "#print(train_data.shape)\n",
    "#print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_half = data_frame.index[data_frame['Timestamp'] == '20210630_2350-0000'].tolist()[0]\n",
    "train_data = data_frame[:18064]\n",
    "val_data = data_frame[18064:26000]\n",
    "test_data = data_frame[idx_half+1:]\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['Timestamp'],axis=1)\n",
    "test_data = test_data.drop(['Timestamp'],axis=1)\n",
    "val_data = val_data.drop(['Timestamp'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = train_data.values\n",
    "test_values = test_data.values\n",
    "val_values = val_data.values\n",
    "\n",
    "train_data = train_values[:, 0:-1]\n",
    "test_data = test_values[:, 0:-1]\n",
    "val_data = val_values[:, 0:-1]\n",
    "train_labels = train_values[:, -1]\n",
    "test_labels = test_values[:, -1]\n",
    "val_labels = val_values[:, -1]\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(val_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "min_val = tf.reduce_min(train_data)\n",
    "max_val = tf.reduce_max(train_data)\n",
    "\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "test_data = (test_data - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)\n",
    "\n",
    "train_data = tf.cast(train_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "val_data = tf.cast(val_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.astype(bool)\n",
    "print(train_labels)\n",
    "test_labels = test_labels.astype(bool)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train_data = train_data[~train_labels]\n",
    "print(f\"정상 데이터(Train)의 shape: {normal_train_data.shape}\")\n",
    "normal_test_data = test_data[~test_labels]\n",
    "print(f\"정상 데이터(Test)의 shape: {normal_test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_train_data = train_data[train_labels]\n",
    "print(f\"비정상 데이터(Train)의 shape: {anomalous_train_data.shape}\")\n",
    "anomalous_test_data = test_data[test_labels]\n",
    "print(f\"비정상 데이터(Test)의 shape: {anomalous_test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 빌드\n",
    "class AnomalyDetector(Model):\n",
    "    def __init__(self):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(16, activation=\"relu\"),\n",
    "            layers.Dense(8, activation=\"relu\")])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(1, activation=\"relu\"),\n",
    "            layers.Dense(2, activation=\"relu\"),\n",
    "            layers.Dense(2, activation=\"sigmoid\")])\n",
    "\n",
    "    def call(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 훈련은 정상 데이터로만 훈련, 테스트는 비정상도 포함\n",
    "history = autoencoder.fit(normal_train_data, normal_train_data, \n",
    "          epochs=30, \n",
    "          batch_size=128,\n",
    "          validation_data=(val_data, val_data),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 plot\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 세트에서 정상에 대한 평균 오차를 계산, 재구성 오류 plot\n",
    "reconstructions = autoencoder.predict(val_data)\n",
    "train_loss = tf.keras.losses.mae(val_data, reconstructions)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "    reconstructions = model(data)\n",
    "    loss = tf.keras.losses.mae(reconstructions, data)\n",
    "    return tf.math.less(loss, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(autoencoder, test_data, threshold)\n",
    "preds = ~preds\n",
    "preds = np.array(preds)\n",
    "preds = preds.astype(float)\n",
    "print(Counter(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(preds, columns=['Prediction'])\n",
    "preds.to_csv('IP_answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5265d7cb68e13c07438725dedd7a5641350d35a25e4a8e23ecc5008449ad4071"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
