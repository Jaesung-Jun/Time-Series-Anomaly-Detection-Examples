{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from dataload_merlion import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "nab_dataset = dataset.load_NAB(\"realTraffic\", \"../Datasets\")\n",
    "nab_dataset_x = nab_dataset[0][0]\n",
    "nab_dataset_y = nab_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nab_dataset_x['value'].to_numpy()\n",
    "labels = nab_dataset_y['anomaly'].to_numpy()\n",
    "\n",
    "data = tf.cast(data, dtype=tf.float32)\n",
    "\n",
    "mean_data = tf.math.reduce_mean(data)\n",
    "std_data = tf.math.reduce_std(data)\n",
    "data = (data - mean_data) / std_data\n",
    "data = tf.cast(data, tf.float32)\n",
    "data = data.numpy()\n",
    "\n",
    "train_data,  test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.3, random_state=21\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataset(data, window_size):\n",
    "        org_shape = data.shape\n",
    "        truncated = data.size - (int(data.size/window_size) * window_size)\n",
    "        data = data[:data.size - truncated]\n",
    "            \n",
    "        dataset = np.empty((int(data.size/window_size), window_size))\n",
    "        j = 0\n",
    "        for i in [n*window_size for n in range(int(data.size/window_size))]:\n",
    "            dataset[j] = data[i:i+window_size]\n",
    "            j = j + 1\n",
    "            \n",
    "        print(f\"Dataset shape changed {org_shape} -> {dataset.shape}.\")\n",
    "        print(f\"Truncated {truncated} values \")\n",
    "        #print(f\"{data[data.size-100:data.size-50]}, {dataset[-2]}\")\n",
    "        #print(f\"{data[data.size-50:data.size]}, {dataset[-1]}\")\n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_overlap_dataset(data, window_size):\n",
    "        org_shape = data.shape\n",
    "\n",
    "        dataset = np.empty((data.size-window_size+1, window_size))\n",
    "        j = 0\n",
    "        for i in range(data.size-window_size+1):\n",
    "            dataset[j] = data[i:i+window_size]\n",
    "            j = j + 1\n",
    "        print(f\"Dataset shape changed {org_shape} -> {dataset.shape}\")\n",
    "        #print(f\"{data[data.size-100:data.size-50]}, {dataset[-2]}\")\n",
    "        #print(f\"{data[data.size-50:data.size]}, {dataset[-1]}\")\n",
    "        return dataset\n",
    "    @staticmethod\n",
    "    def create_labels(labels, window_size):\n",
    "        new_labels = []\n",
    "        for i in range(len(labels)-window_size+1):\n",
    "            if True in labels[i:i+window_size]:\n",
    "                new_labels.append(True)\n",
    "            else:\n",
    "                new_labels.append(False)\n",
    "                \n",
    "        return np.array(new_labels)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_overlap_labels(labels, window_size):\n",
    "        new_labels = []\n",
    "        for i in range(len(labels)-window_size+1):\n",
    "            new_labels.append(labels[i:i+window_size])\n",
    "                \n",
    "        return np.array(new_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 30\n",
    "\n",
    "train_data = Preprocessing.create_overlap_dataset(train_data, WINDOW_SIZE)\n",
    "test_data = Preprocessing.create_overlap_dataset(test_data, WINDOW_SIZE)\n",
    "\n",
    "train_labels = train_labels.astype(bool)\n",
    "test_labels = test_labels.astype(bool)\n",
    "\n",
    "cutted_train_labels = Preprocessing.create_labels(train_labels, WINDOW_SIZE)\n",
    "cutted_test_labels = Preprocessing.create_labels(test_labels, WINDOW_SIZE)\n",
    "\n",
    "normal_train_data = train_data[cutted_train_labels]\n",
    "normal_test_data = test_data[cutted_test_labels]\n",
    "\n",
    "anomalous_test_data = np.concatenate((test_data[~cutted_test_labels], train_data[~cutted_train_labels]))\n",
    "\n",
    "print(f\"normal train data size : {len(normal_train_data)}\")\n",
    "print(f\"normal test data size : {len(normal_test_data)}\")\n",
    "\n",
    "#print(f\"anomalous train data size : {len(anomalous_train_data)}\")\n",
    "print(f\"anomalous test data size : {len(anomalous_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(30), normal_train_data[0])\n",
    "plt.title(\"Normal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(30), anomalous_test_data[0])\n",
    "plt.title(\"Anomalous\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(30, activation=\"relu\"),\n",
    "      layers.Dense(20, activation=\"relu\"),\n",
    "      layers.Dense(10, activation=\"relu\"),\n",
    "      layers.Dense(8, activation=\"relu\")])\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(10, activation=\"relu\"),\n",
    "      layers.Dense(20, activation=\"relu\"),\n",
    "      layers.Dense(30, activation=\"sigmoid\")])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(normal_train_data, normal_train_data, \n",
    "          epochs=30,\n",
    "          batch_size=16,\n",
    "          validation_data=(test_data, test_data),\n",
    "          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX=0\n",
    "encoded_imgs = autoencoder.encoder(normal_test_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(normal_test_data[INDEX], 'b')\n",
    "plt.plot(decoded_imgs[INDEX], 'r')\n",
    "plt.fill_between(np.arange(30), decoded_imgs[INDEX], normal_test_data[INDEX], color='lightcoral')\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()\n",
    "mse = mean_squared_error(decoded_imgs, normal_test_data)\n",
    "print(\"Mean Squared Error : \", mse)\n",
    "mse_list = []\n",
    "for i in range(len(decoded_imgs)):\n",
    "    mse_list.append(mean_squared_error(decoded_imgs[i], normal_test_data[i]))\n",
    "print(max(mse_list))\n",
    "print(mse_list.index(max(mse_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX=0\n",
    "\n",
    "encoded_imgs = autoencoder.encoder(anomalous_test_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(anomalous_test_data[INDEX], 'b')\n",
    "plt.plot(decoded_imgs[INDEX], 'r')\n",
    "plt.fill_between(np.arange(30), decoded_imgs[INDEX], anomalous_test_data[INDEX], color='lightcoral')\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()\n",
    "mse = mean_squared_error(decoded_imgs, anomalous_test_data)\n",
    "print(\"Mean Squared Error : \", mse)\n",
    "mse_list = []\n",
    "for i in range(len(decoded_imgs)):\n",
    "    mse_list.append(mean_squared_error(decoded_imgs[i], anomalous_test_data[i]))\n",
    "print(max(mse_list))\n",
    "print(mse_list.index(max(mse_list)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20031c11293a90a3e10975bd99452ccbfe59e9ec1150720a358013292819162b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
